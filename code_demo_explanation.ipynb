{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007930b4",
   "metadata": {},
   "source": [
    "# Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00a220",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "Minimum requirements to run Colossal AI\n",
    "\n",
    "- PyTorch >= 1.11 and PyTorch <= 2.1\n",
    "- Python >= 3.7\n",
    "- CUDA >= 11.0\n",
    "- NVIDIA GPU Compute Capability >= 7.0 (V100/RTX20 and higher)\n",
    "- Linux OS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe7c43",
   "metadata": {},
   "source": [
    "**Now, let's dive into the code and walk through each step of the implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa964b",
   "metadata": {},
   "source": [
    "### Import Required Packages\n",
    "\n",
    "In this section, we start by importing the necessary packages and libraries required for our code implementation:\n",
    "\n",
    "- `os`: Allows interaction with the operating system, such as setting environment variables which will be used for ColossalAI.\n",
    "- `time`: Provides functions for working with time-related tasks, which may be useful for timing experiments.\n",
    "- `warnings`: Enables handling of warning messages to control their display.\n",
    "- `torch`: The core library for all tensor computations and neural network operations in PyTorch.\n",
    "- `torch.distributed`: Facilitates distributed computing, crucial for parallel training across multiple GPUs.\n",
    "- `torch.nn`: Contains neural network modules, such as layers, activations, loss functions, etc.\n",
    "- `torchvision`: Offers popular datasets, model architectures, and image transformations for computer vision tasks.\n",
    "- `torch.optim`: Provides optimization algorithms, such as Adam, SGD, etc., for training neural networks.\n",
    "- `torch.nn.functional`: Includes functional interface for operations like convolution, activation functions, etc.\n",
    "- `torchvision.transforms`: Contains image transformation functions for data augmentation and preprocessing.\n",
    "- `torch.optim.Optimizer`: Base class for all optimizers in PyTorch, used for defining custom optimizers.\n",
    "- `torch.optim.lr_scheduler.MultiStepLR`: Scheduler for learning rate updates based on predefined milestones.\n",
    "- `torch.utils.data.DataLoader`: Allows efficient loading of data in batches, essential for training neural networks.\n",
    "- `colossalai`: The core library for Colossal-AI, providing tools and components for large-scale parallel training.\n",
    "- `colossalai.accelerator`: Provides functions for interacting with the hardware accelerator, such as GPUs.\n",
    "- `colossalai.booster`: Offers functionalities for boosting model training, including distributed data parallelism.\n",
    "- `colossalai.booster.plugin.TorchDDPPlugin`: Plugin for integrating Colossal-AI with PyTorch's Distributed Data Parallel (DDP).\n",
    "- `colossalai.cluster.DistCoordinator`: Coordinates distributed computations across multiple nodes.\n",
    "- `colossalai.nn.optimizer.HybridAdam`: Optimizer that combines local and global gradient updates for efficient training.\n",
    "- `matplotlib.pyplot`: Library for creating static, interactive, and animated visualizations in Python.\n",
    "- `seaborn`: Provides high-level interface for drawing attractive and informative statistical graphics.\n",
    "- `numpy`: Fundamental package for scientific computing with Python, used for numerical operations.\n",
    "- `torchmetrics.classification.MulticlassROC`: Metric for computing ROC curves in multi-class classification tasks.\n",
    "- `sklearn.metrics`: Offers functions for evaluating model performance, such as precision, recall, etc.\n",
    "\n",
    "We also use `warnings.filterwarnings(\"ignore\")` to suppress warning messages for cleaner output during execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626ecf3",
   "metadata": {},
   "source": [
    "### Set up distributed training environment variables\n",
    "\n",
    "In this section, we set up environment variables required for distributed training:\n",
    "\n",
    "- `os.environ[\"RANK\"] = \"0\"`: Specifies the rank of the current process in the distributed environment. Here, we set it to 0, indicating it as the first process.\n",
    "- `os.environ[\"LOCAL_RANK\"] = \"0\"`: Defines the local rank of the current process, which is also set to 0 in this case.\n",
    "- `os.environ[\"WORLD_SIZE\"] = \"1\"`: Specifies the total number of processes participating in the distributed training. Since we're not distributing the training in this demo, the world size is set to 1.\n",
    "- `os.environ[\"MASTER_ADDR\"] = \"localhost\"`: Sets the address of the master node, which is the local machine in this case, indicated by \"localhost\".\n",
    "- `os.environ[\"MASTER_PORT\"] = \"1234\"`: Specifies the port for communication between different processes in the distributed environment, which is set to 1234 for this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92208d0d",
   "metadata": {},
   "source": [
    "### Set device\n",
    "\n",
    "In this section, we determine the device on which our computations will be performed:\n",
    "\n",
    "- `torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")`: Creates a PyTorch device object based on the availability of CUDA. If CUDA is available, the device is set to GPU (\"cuda\"); otherwise, it defaults to CPU (\"CPU\").\n",
    "\n",
    "This step ensures that our code can seamlessly switch between GPU and CPU based on hardware availability, optimizing performance and resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7867ed2",
   "metadata": {},
   "source": [
    "### Define Hyperparameters\n",
    "\n",
    "In this section, we define the hyperparameters required for training our neural network model:\n",
    "\n",
    "- `BATCH_SIZE`: Specifies the number of samples in each batch during training. A larger batch size can lead to faster convergence but may require more memory.\n",
    "- `NUM_EPOCHS`: Indicates the number of times the entire dataset will be passed forward and backward through the neural network during training. Each epoch consists of multiple iterations (batches).\n",
    "- `LEARNING_RATE`: Defines the rate at which the model's parameters are updated during optimization. It determines the step size for gradient descent and impacts the convergence speed and final performance of the model.\n",
    "- `GAMMA`: Represents the factor by which to reduce the learning rate at each milestone specified in the learning rate scheduler. It is used to decay the learning rate over time, enabling finer control over the optimization process.\n",
    "\n",
    "These hyperparameters play a crucial role in determining the performance and convergence behavior of our neural network model during training. Adjusting them appropriately is essential for achieving optimal results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723af60",
   "metadata": {},
   "source": [
    "### Initialize ColossalAI, Plugin, and Booster\n",
    "\n",
    "In this section, we initialize ColossalAI, along with its plugin and booster components:\n",
    "\n",
    "- `colossalai.launch_from_torch(config={})`: Launches ColossalAI from PyTorch, enabling seamless integration. This step ensures that ColossalAI's functionalities are available for use in our code. Please comment out this line if ColossalAI has already been launched to avoid redundancy.\n",
    "- `coordinator = DistCoordinator()`: Creates a distributed coordinator object responsible for coordinating distributed computations across multiple nodes. It manages communication between processes during parallel training.\n",
    "- `plugin = TorchDDPPlugin()`: Initializes a plugin for integrating ColossalAI with PyTorch's Distributed Data Parallel (DDP) module. This plugin facilitates efficient distributed training by managing data parallelism across multiple GPUs.\n",
    "- `booster = Booster(plugin=plugin)`: Initializes a booster object, which serves as the main interface for utilizing ColossalAI's capabilities. It is configured with the previously created plugin to leverage distributed data parallelism for enhanced model training.\n",
    "\n",
    "These initializations lay the foundation for leveraging ColossalAI's features, enabling efficient large-scale parallel training of neural network models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e43b35",
   "metadata": {},
   "source": [
    "### Transformation and Data Loaders\n",
    "\n",
    "In this section, we define transformations, download dataset and create data loaders for our dataset:\n",
    "\n",
    "#### Transformations:\n",
    "- `transform_train`: Composes a series of transformations for the training dataset, including converting images to tensors and normalizing pixel values.\n",
    "- `transform_test`: Composes transformations for the testing dataset, similar to the training set.\n",
    "\n",
    "#### MNIST Dataset:\n",
    "- We download the MNIST dataset provided by torchvision library and apply the defined transformations. The dataset consists of handwritten digits from 0 to 9, with separate training and testing splits. We'll preprocess and load the dataset using PyTorch's DataLoader, making it ready for model training.\n",
    "\n",
    "#### Data Loaders:\n",
    "- `train_dataloader`: Constructs a data loader for the training dataset, which iterates over batches of data during training. It shuffles the data and drops the last incomplete batch.\n",
    "- `test_dataloader`: Creates a data loader for the testing dataset, ensuring that batches are not shuffled or dropped.\n",
    "\n",
    "#### MNIST Dataset for ColossalAI:\n",
    "- To demonstrate ColossalAI's compatibility, we prepare another set of MNIST data specifically for use with ColossalAI. This involves downloading the dataset and applying transformations within the ColossalAI framework.\n",
    "\n",
    "#### Data Loader for ColossalAI:\n",
    "- `train_dataloader_Col`: Prepares a data loader for the ColossalAI-compatible training dataset, utilizing ColossalAI's data preparation capabilities to ensure efficient parallel training.\n",
    "- `test_dataloader_Col`: Constructs a data loader for the testing dataset compatible with ColossalAI.\n",
    "\n",
    "These transformations and data loaders are essential components for efficiently handling and processing our dataset during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f0e18",
   "metadata": {},
   "source": [
    "### Grayscale or RGB Image Check\n",
    "\n",
    "In this section, we perform a check to determine if the images in our dataset are grayscale or RGB.\n",
    "\n",
    "If the number of channels is 1, the images are grayscale.<br/>\n",
    "If the number of channels is 3, the images are RGB.</br>\n",
    "This check helps ensure that our model architecture and data preprocessing are aligned with the expected input format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfada5c",
   "metadata": {},
   "source": [
    "### Sample Images\n",
    "\n",
    "In this section, we visualize a batch of sample images from the dataset:\n",
    "\n",
    "- `classes = train_dataset.classes`: Retrieves the list of classes (labels) present in the dataset.\n",
    "- `imshow`: Defines a function to display images.\n",
    "- `dataiter = iter(train_dataloader)`: Creates an iterator over the training data loader.\n",
    "- `images, labels = next(dataiter)`: Retrieves the next batch of images and labels from the iterator.\n",
    "- `imshow(torchvision.utils.make_grid(images))`: Displays the batch of images in a grid format using the `imshow` function.\n",
    "- `print(' '.join(f'{classes[labels[j]]:5s}' for j in range(BATCH_SIZE)))`: Prints the corresponding labels for each image in the batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa97bc",
   "metadata": {},
   "source": [
    "### Define Model Architecture\n",
    "\n",
    "In this section, we define the architecture of the VGG13 neural network model:\n",
    "\n",
    "- `class VGG13(nn.Module)`: Defines a class named VGG13 that inherits from the `nn.Module` class, making it a PyTorch neural network module.\n",
    "- `def __init__(self, dropoutAdd=False)`: Initializes the VGG13 model with optional dropout layers.\n",
    "- `Convolutional layers`: Defines five sets of convolutional layers followed by ReLU activation functions and batch normalization.\n",
    "- `Fully-connected layers`: Defines two fully-connected layers with ReLU activation and optional dropout layers.\n",
    "- `def forward(self, x)`: Implements the forward pass of the model, defining the sequence of operations applied to input data `x`.\n",
    "- `modelVGG = VGG13(dropoutAdd=True).to(device)`: Creates an instance of the VGG13 model with dropout layers enabled, and moves it to the specified device (GPU or CPU).\n",
    "- `print(\"\\nModel Architecture:\\n\")`: Prints a message indicating the display of the model architecture.\n",
    "- `print(modelVGG)`: Prints the architecture of the VGG13 model, displaying the sequential arrangement of layers and their parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8ea49",
   "metadata": {},
   "source": [
    "### Prepare Criterion, Optimizer, Learning Rate Scheduler\n",
    "\n",
    "In this section, we prepare the criterion, optimizer, and learning rate scheduler for model training:\n",
    "\n",
    "- `criterion = nn.CrossEntropyLoss()`: Defines the criterion for calculating the loss, which is the cross-entropy loss suitable for classification tasks.\n",
    "- `optimizer = optim.Adam(modelVGG.parameters(), lr=LEARNING_RATE)`: Initializes the Adam optimizer with the VGG13 model parameters and the specified learning rate.\n",
    "- `optimizer_Col = HybridAdam(modelVGG.parameters(), lr=LEARNING_RATE)`: Initializes the ColossalAI-specific HybridAdam optimizer with the VGG13 model parameters and the specified learning rate.\n",
    "- `lr_scheduler = MultiStepLR(optimizer, milestones=[20, 40, 60, 80], gamma=GAMMA)`: Initializes a multi-step learning rate scheduler, which adjusts the learning rate at predefined milestones during training, with a decay factor defined by `GAMMA`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ff0ce",
   "metadata": {},
   "source": [
    "### Define Model Training\n",
    "\n",
    "The `modelTraining` function trains the neural network model for a specified number of epochs. It performs both training and testing phases, computes and accumulates the losses, and adjusts the model parameters through backpropagation and optimization. Here's a breakdown of its components:\n",
    "\n",
    "- The function takes input parameters such as the number of epochs, the model architecture, optimizer, criterion (loss function), learning rate scheduler, data loaders for training and testing, as well as ColossalAI-specific objects if applicable.\n",
    "- It initializes arrays to store training and testing loss values throughout the epochs.\n",
    "- For each epoch, it iterates through the training data, computes the loss, and updates the model parameters based on the optimizer.\n",
    "- It then evaluates the model on the testing data to compute the testing loss.\n",
    "- The function also prints log information such as the current epoch, training loss, and testing loss every two epochs.\n",
    "- Finally, it returns the arrays containing the training and testing loss values and prints the total time taken for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d3314",
   "metadata": {},
   "source": [
    "### Plot Losses\n",
    "\n",
    "This function generates a plot displaying the training and testing losses across epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b719262",
   "metadata": {},
   "source": [
    "### Define Model Testing\n",
    "\n",
    "This section includes functions for evaluating the model's performance on the test dataset, including accuracy, precision, recall, F-score, confusion matrix, and ROC curve.\n",
    "\n",
    "#### `loadersAccuracy(model, loader, colossalAI)`\n",
    "\n",
    "This function calculates the accuracy of the model on the given dataloader.\n",
    "\n",
    "- It iterates through the data loader, computes the model's predictions, and compares them with the ground truth labels to calculate the number of correct predictions.\n",
    "- For colossalAI, it utilizes distributed computing to aggregate correct and total counts across all processes.\n",
    "- It also collects true labels and predicted labels for further evaluation.\n",
    "\n",
    "#### `perfEvaluation(model, train_dataloader, test_dataloader, colossalAI = False)`\n",
    "\n",
    "This function evaluates the model's performance on the training and testing datasets.\n",
    "\n",
    "- It computes the accuracy, precision, recall, and F-score on the testing dataset and prints the results.\n",
    "- It also plots the accuracies on both training and testing datasets using a bar chart.\n",
    "- Additionally, it generates a confusion matrix to visualize the model's performance across different classes.\n",
    "- Finally, it plots the ROC curve to assess the model's ability to discriminate between different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a7f12",
   "metadata": {},
   "source": [
    "### Model Evaluation without Colossal AI\n",
    "\n",
    "This section evaluates the model's performance without using Colossal AI for distributed training.\n",
    "\n",
    "- It then calls the `modelTraining` function to train the VGG model for the specified number of epochs without utilizing Colossal AI. The training and testing losses are plotted using the `LossesPlot` function.\n",
    "\n",
    "- Finally, the `perfEvaluation` function evaluates the model's performance on both the training and testing datasets without Colossal AI. It prints the accuracy, precision, recall, and F-score, and generates visualizations including a bar chart of accuracies, a confusion matrix, and an ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27050e",
   "metadata": {},
   "source": [
    "### Boost with ColossalAI\n",
    "\n",
    "This step involves boosting the model with ColossalAI for distributed training.\n",
    "\n",
    "- The `booster.boost` function is called to boost the VGG model with ColossalAI. This function returns the boosted model (`model_Col`), boosted optimizer (`optimizer_Col`), boosted criterion (`criterion_Col`), along with other objects like the learning rate scheduler (`lr_scheduler_Col`).\n",
    "\n",
    "- The boosted model (`model_Col`) is trained using ColossalAI for the specified number of epochs.\n",
    "\n",
    "- The boosted model's performance is evaluated using the `perfEvaluation` function, which calculates metrics such as accuracy, precision, recall, and F-score, and generates visualizations including a bar chart of accuracies, a confusion matrix, and an ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2d5a2",
   "metadata": {},
   "source": [
    "### Model Evaluation with Colossal AI\n",
    "\n",
    "This section involves evaluating the model trained with ColossalAI for distributed training.\n",
    "\n",
    "- The boosted model (`model_Col`) trained with ColossalAI is evaluated for the specified number of epochs.\n",
    "\n",
    "- The `modelTraining` function is called with the ColossalAI flag set to `True`, indicating that the training is performed using ColossalAI.\n",
    "\n",
    "- The training and testing losses are plotted using the `LossesPlot` function.\n",
    "\n",
    "- The performance of the model is evaluated using the `perfEvaluation` function, which calculates metrics such as accuracy, precision, recall, and F-score, and generates visualizations including a bar chart of accuracies, a confusion matrix, and an ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71e83e",
   "metadata": {},
   "source": [
    "### Time to Train\n",
    "\n",
    "Below are the time duration taken to train the model with and without ColossalAI.\n",
    "\n",
    "- `Without ColossalAI`: 8 minutes and 21 seconds.\n",
    "- `With ColossalAI`: 8 minutes and 4 seconds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e1112",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "`Efficiency Gains Over Multiple Epochs`:\n",
    "Training the model with Colossal-AI proved to be slightly quicker compared to traditional methods over 20 epochs, indicating that Colossal-AI could potentially increase training speed, although the time saved was small.\n",
    "\n",
    "`Impact of Dataset Simplicity on Parallel Training`:\n",
    "Using VGG13 with the simple MNIST dataset didn't fully demonstrate the benefits of parallel training, which typically becomes more apparent with larger or more complex datasets like Cifar-100 and models that require more computing power.\n",
    "\n",
    "`Hardware Limitations and Parallelization Efficiency`:\n",
    "Since only two GPUs were used, we couldn't fully leverage the advantages of Colossal-AI's parallel training features. Colossal-AI is more effective with more extensive GPU setups, where without enough hardware, the setup and synchronization efforts can diminish the benefits of faster processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61652e51",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "`Below are the references used throughout Code Demo.`\n",
    "\n",
    "1. https://numpy.org/doc/\n",
    "1. https://matplotlib.org/stable/index.html\n",
    "1. https://scikit-learn.org/stable/\n",
    "1. https://seaborn.pydata.org/\n",
    "1. https://pytorch.org/tutorials/\n",
    "1. https://pytorch.org/vision/main/models/vision_transformer.html\n",
    "1. https://colossalai.org/docs/get_started/installation/\n",
    "1. https://github.com/hpcaitech/ColossalAI\n",
    "1. https://github.com/hpcaitech/ColossalAI/blob/main/examples/images/resnet/train.py\n",
    "1. Data Loading, VGG13 Model Architecture, Model Training and Testing is based on CSE 676 Deep Learning Assignment 1 Part 1 and Bonus submission by Nikhil Gupta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa1f82",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "For the task, your challenge is to:\n",
    "\n",
    "- Modify the code to use the CIFAR-10 dataset instead of MNIST.\n",
    "- Update the VGG-13 model architecture and hyperparameters according to CIFAR-10 requirements.\n",
    "- After completing the task, share your experiences and insights in the notebook file itself. \n",
    "- You can document any challenges faced, improvements made, or interesting observations during the process. \n",
    "\n",
    "Feel free to ask questions or discuss your findings in the piazza discussion forum. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
